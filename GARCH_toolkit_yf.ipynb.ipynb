{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# ğŸ“‰ Financial Volatility Modeling â€” GARCH Toolkit  \n**by Lyra**\n\n> å®æˆ˜å–å‘ï¼šæ”¹å‚æ•°å³å¯å¤ç”¨ï¼›é»˜è®¤ yfinanceï¼Œæ— éœ€ tokenã€‚å¯é€‰ TuShareï¼ˆéœ€ tokenï¼‰ä¸ CSVã€‚\n\nOutputs: `./figures/`ï¼ˆå›¾ï¼‰ä¸ `./outputs/`ï¼ˆé¢„æµ‹/æŒ‡æ ‡ï¼‰ã€‚"}, {"cell_type": "markdown", "metadata": {}, "source": "## 0. Parametersï¼ˆä»…éœ€ä¿®æ”¹æœ¬èŠ‚å³å¯å¤ç”¨ï¼‰"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n# ---- Edit here ----\nSYMBOL = \"600036.SS\"   # æ‹›å•†é“¶è¡Œï¼šYahoo æ ¼å¼ï¼›ä¹Ÿå¯ AAPLã€0700.HKã€600036.SS ç­‰\nSTART  = \"2018-01-01\"\nEND    = \"2025-01-01\"\n\nDATA_SOURCE = \"yfinance\"   # \"yfinance\"ï¼ˆé»˜è®¤ï¼‰| \"tushare\" | \"csv\"\nCSV_PATH    = \"zhaoshang.csv\"  # ä»…åœ¨ DATA_SOURCE=\"csv\" æ—¶ç”Ÿæ•ˆ\n\n# GARCH specification\nMEAN_MODEL = \"Constant\"    # \"Zero\" | \"Constant\" | \"AR(1)\"\nVOL_MODEL  = \"GARCH\"       # \"ARCH\" | \"GARCH\"\nP, Q       = 1, 1          # ARCH/GARCH é˜¶æ•°ï¼ˆå¸¸ç”¨ GARCH(1,1)ï¼‰\nDIST       = \"normal\"      # \"normal\" | \"t\"ï¼ˆt åˆ†å¸ƒï¼‰\n\n# Forecast horizon (days)\nFORECAST_STEPS = 20\n\n# Output folders\nFIG_DIR = \"figures\"\nOUT_DIR = \"outputs\"\n# --------------------\n\nimport os, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom pathlib import Path\n\npd.options.display_float_format = \"{:.6f}\".format\nplt.rcParams[\"figure.figsize\"] = (10, 6)\n\nPath(FIG_DIR).mkdir(parents=True, exist_ok=True)\nPath(OUT_DIR).mkdir(parents=True, exist_ok=True)\n\nprint(\"SYMBOL:\", SYMBOL, \"| Window:\", START, \"â†’\", END, \"| Source:\", DATA_SOURCE)\nprint(\"Spec:\", f\"{MEAN_MODEL} + {VOL_MODEL}({P},{Q})\", \"| Dist:\", DIST, \"| Forecast:\", FORECAST_STEPS, \"d\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 1. æ•°æ®è¯»å–ï¼ˆé»˜è®¤ yfinanceï¼›å¯é€‰ TuShare æˆ– CSVï¼‰"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndef load_yfinance(symbol, start, end):\n    try:\n        import yfinance as yf\n    except Exception:\n        raise ImportError(\"ç¼ºå°‘ yfinanceï¼Œè¯·å…ˆå®‰è£…ï¼špip install yfinance\")\n    data = yf.download(symbol, start=start, end=end, progress=False)\n    if data is None or len(data)==0:\n        raise ValueError(\"yfinance æœªè¿”å›æ•°æ®ï¼Œè¯·æ£€æŸ¥ä»£ç ä¸æ—¥æœŸèŒƒå›´ã€‚\")\n    df = data.reset_index()[[\"Date\",\"Adj Close\"]].rename(columns={\"Date\":\"datetime\",\"Adj Close\":\"close\"})\n    return df\n\ndef load_tushare(symbol, start, end):\n    import os\n    token = os.getenv(\"TUSHARE_TOKEN\", os.getenv(\"TS_TOKEN\", \"\")).strip()\n    if not token:\n        raise ValueError(\"ç¼ºå°‘ TuShare Tokenï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ TUSHARE_TOKENã€‚\")\n    try:\n        import tushare as ts\n    except Exception:\n        raise ImportError(\"ç¼ºå°‘ tushareï¼Œè¯·å…ˆå®‰è£…ï¼špip install tushare\")\n    ts.set_token(token)\n    pro = ts.pro_api(token)\n    s = pd.Timestamp(start).strftime(\"%Y%m%d\")\n    e = pd.Timestamp(end).strftime(\"%Y%m%d\")\n    ts_code = symbol if symbol.endswith((\".SH\",\".SZ\")) else symbol\n    df = pro.daily(ts_code=ts_code, start_date=s, end_date=e)\n    if df is None or len(df)==0:\n        raise ValueError(\"TuShare æœªè¿”å›æ•°æ®ï¼Œè¯·æ£€æŸ¥ ts_code ä¸æ—¥æœŸã€‚\")\n    df[\"trade_date\"] = pd.to_datetime(df[\"trade_date\"], format=\"%Y%m%d\", errors=\"coerce\")\n    df = df.rename(columns={\"trade_date\":\"datetime\", \"close\":\"close\"})[[\"datetime\",\"close\"]]\n    df = df.sort_values(\"datetime\").reset_index(drop=True)\n    return df\n\ndef load_csv(csv_path):\n    df = pd.read_csv(csv_path)\n    for c in [\"datetime\",\"trade_date\",\"date\",\"Date\"]:\n        if c in df.columns:\n            date_col = c; break\n    else:\n        raise ValueError(\"CSV éœ€åŒ…å«æ—¥æœŸåˆ—ï¼šdatetime/trade_date/date/Date\")\n    if date_col == \"trade_date\":\n        df[date_col] = pd.to_datetime(df[date_col].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n    else:\n        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    for c in [\"close\",\"Close\",\"Adj Close\",\"adj_close\",\"AdjClose\"]:\n        if c in df.columns:\n            price_col = c; break\n    else:\n        raise ValueError(\"CSV éœ€åŒ…å«ä»·æ ¼åˆ—ï¼šclose/Adj Close ç­‰\")\n    df = df[[date_col, price_col]].rename(columns={date_col:\"datetime\", price_col:\"close\"})\n    df = df.sort_values(\"datetime\").dropna().reset_index(drop=True)\n    return df\n\nif DATA_SOURCE == \"yfinance\":\n    raw = load_yfinance(SYMBOL, START, END)\nelif DATA_SOURCE == \"tushare\":\n    raw = load_tushare(SYMBOL, START, END)\nelif DATA_SOURCE == \"csv\":\n    raw = load_csv(CSV_PATH)\nelse:\n    raise ValueError(\"DATA_SOURCE å¿…é¡»ä¸º 'yfinance' | 'tushare' | 'csv'\")\n\nraw = raw[(raw[\"datetime\"]>=START) & (raw[\"datetime\"]<=END)].copy().sort_values(\"datetime\")\nprint(\"Data window:\", raw[\"datetime\"].min().date(), \"â†’\", raw[\"datetime\"].max().date(), \"| points:\", len(raw))\nraw.head()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. è®¡ç®—æ”¶ç›Šç‡ï¼ˆLog Returnï¼‰ä¸åŸºç¡€ç»Ÿè®¡"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndf = raw.copy()\ndf[\"ret\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\nret = df[\"ret\"].dropna()\n\ndaily_mu, daily_sig = ret.mean(), ret.std(ddof=1)\nann_mu, ann_vol = daily_mu*252, daily_sig*np.sqrt(252)\n\nprint(f\"Daily mean: {daily_mu:.6f}, Daily vol: {daily_sig:.6f}\")\nprint(f\"Annualized return: {ann_mu:.4%}, Annualized volatility: {ann_vol:.4%}\")\nret.describe()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 3. æ¨¡å‹è®¾å®šä¸æ‹Ÿåˆï¼ˆARCH / GARCHï¼‰"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ntry:\n    from arch import arch_model\nexcept Exception:\n    raise ImportError(\"ç¼ºå°‘ arch åŒ…ï¼Œè¯·å…ˆå®‰è£…ï¼špip install arch\")\n\nmean = MEAN_MODEL.lower()\nif mean not in (\"zero\",\"constant\",\"ar(1)\"):\n    raise ValueError(\"MEAN_MODEL ä»…æ”¯æŒ 'Zero' | 'Constant' | 'AR(1)'\")\n\nvol = VOL_MODEL.upper()\nif vol not in (\"ARCH\",\"GARCH\"):\n    raise ValueError(\"VOL_MODEL ä»…æ”¯æŒ 'ARCH' | 'GARCH'\")\n\ndist = DIST.lower()\nif dist not in (\"normal\",\"t\"):\n    raise ValueError(\"DIST ä»…æ”¯æŒ 'normal' æˆ– 't'\")\n\nam = arch_model(\n    ret * 100,\n    mean=\"Zero\" if mean==\"zero\" else (\"Constant\" if mean==\"constant\" else \"ARX\"),\n    lags=1 if mean==\"ar(1)\" else 0,\n    vol=vol,\n    p=P, q=Q,\n    dist=dist\n)\nres = am.fit(update_freq=10, disp=\"off\")\nprint(res.summary())\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4. æ¡ä»¶æ³¢åŠ¨ç‡ï¼ˆConditional Volatilityï¼‰å¯è§†åŒ–"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ncond_vol = res.conditional_volatility / 100.0 * np.sqrt(252)\nvol_ts = pd.Series(cond_vol, index=ret.index)\n\nfig = plt.figure()\nplt.plot(vol_ts.index, vol_ts.values, label=\"Annualized Conditional Volatility\")\nplt.title(\"Annualized Conditional Volatility (GARCH)\")\nplt.legend(); plt.grid(True); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/garch_conditional_vol_annualized.png\", dpi=200)\nplt.show()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 5. æ³¢åŠ¨ç‡é¢„æµ‹ï¼ˆVariance/Volatility Forecastï¼‰"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nfcast = res.forecast(horizon=FORECAST_STEPS, reindex=False)\nvariance = fcast.variance.values[-1]\nvol_forecast = np.sqrt(variance) / 100.0 * np.sqrt(252)\n\nfc_df = pd.DataFrame({\n    \"step\": np.arange(1, FORECAST_STEPS+1),\n    \"annualized_vol\": vol_forecast\n})\nfc_df.to_csv(f\"{OUT_DIR}/forecast_annualized_vol.csv\", index=False, encoding=\"utf-8\")\nfc_df.head()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 6. æ¨¡å‹è¯Šæ–­ï¼ˆæ®‹å·®ä¸åˆ†å¸ƒï¼‰"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nstd_resid = res.std_resid\nfig = plt.figure()\nplt.plot(std_resid, label=\"Standardized Residuals\")\nplt.title(\"Standardized Residuals\")\nplt.legend(); plt.grid(True); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/standardized_residuals.png\", dpi=200)\nplt.show()\n\nfig = plt.figure()\nplt.hist(std_resid.dropna(), bins=60, density=True)\nplt.title(\"Standardized Residuals â€” Distribution\")\nplt.grid(True); plt.tight_layout()\nplt.savefig(f\"{FIG_DIR}/standardized_resid_hist.png\", dpi=200)\nplt.show()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 7. å¯¼å‡ºæ ¸å¿ƒæŒ‡æ ‡"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nsumm = {\n    \"symbol\": SYMBOL,\n    \"start\": START, \"end\": END,\n    \"mean_model\": MEAN_MODEL, \"vol_model\": VOL_MODEL, \"p\": P, \"q\": Q, \"dist\": DIST,\n    \"daily_mean\": float(ret.mean()), \"daily_vol\": float(ret.std(ddof=1)),\n    \"ann_vol_last\": float(vol_ts.dropna().iloc[-1]) if len(vol_ts.dropna())>0 else None,\n    \"forecast_steps\": FORECAST_STEPS,\n    \"note\": \"volatility in annualized terms\"\n}\nimport pandas as pd\npd.DataFrame([summ]).to_csv(f\"{OUT_DIR}/garch_summary.csv\", index=False, encoding=\"utf-8\")\nprint(\"Saved:\", f\"{OUT_DIR}/garch_summary.csv\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## ğŸ’¬ Personal Reflectionï¼ˆä¸ªäººæ€»ç»“ï¼‰\nGARCH çš„æ ¸å¿ƒä»·å€¼åœ¨äºè¯†åˆ«æ³¢åŠ¨èšç±»ä¸é£é™© regimeã€‚æˆ‘çš„åšæ³•æ˜¯ä¼˜å…ˆä¿è¯æµç¨‹å¯å¤ç”¨ï¼šç»Ÿä¸€æ•°æ®æ¥å£ã€å›ºå®šå»ºæ¨¡æ­¥éª¤ã€å¯¼å‡ºæ ‡å‡†åŒ–ç»“æœã€‚å®é™…æŠ•ç ”ä¸­ï¼Œæˆ‘ä¼šæ ¹æ®æ®‹å·®è¯Šæ–­è°ƒæ•´åˆ†å¸ƒä¸é˜¶æ•°ï¼Œè®©æ¨¡å‹æ›´è´´è¿‘èµ„äº§ç‰¹æ€§ã€‚"}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}